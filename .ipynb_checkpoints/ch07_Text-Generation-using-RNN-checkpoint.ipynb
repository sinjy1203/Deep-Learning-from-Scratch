{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a7d4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.functions import softmax\n",
    "from common.other_class import Rnnlm, BetterRnnlm\n",
    "\n",
    "class RnnlmGen(Rnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
    "        word_ids = [start_id]\n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1)\n",
    "            score = self.predict(x)\n",
    "            p = softmax(score.flatten())\n",
    "            \n",
    "            sampled = np.random.choice(len(p), size=1, p=p)\n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))\n",
    "        return word_ids\n",
    "    def get_state(self):\n",
    "        return self.lstm_layer.h, self.lstm_layer.c\n",
    "    def set_state(self, state):\n",
    "        self.lstm_layer.set_state(*state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e3b56",
   "metadata": {},
   "source": [
    "### None Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c6bfdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you doyle mundane smallest kirk seeks margins homes n't polyethylene cell kia improvements mo. grade exempt suez dec. swung torrijos concord spate keating structures crossing willingness rape principles obliged returned pumped nearly evolution effectiveness aggregates rockefeller schemes day-to-day willful cs gartner formal vaccine centers eager files czechoslovakia corporations anything joined killing triple-a quotas curbing plug jerry hazards crack cycling hutton reiterated disposal stone fleischmann exception foot conclude riders affecting gathered accumulation murdoch ddb losing owner functions fla waste climb endless longer-term crystals defendants pressured dealers d.c. invitation disabilities claimed ashland strategists arab defaulted recreational appliances unified chlorofluorocarbons venture-capital activist d'arcy\n"
     ]
    }
   ],
   "source": [
    "import ptb\n",
    "\n",
    "model = RnnlmGen()\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[word] for word in skip_words]\n",
    "\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '\\n')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b2fb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b68561a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you pointed out of the dire mr. roman said\n",
      " both california one of the game is n't so low\n",
      " surprisingly oct.\n",
      " on the american home television institute issued of a moderate buy-back of shares of shares at a team of frankfurt companies\n",
      " react to suspend municipal earnings\n",
      " perhaps deferring short value or work could appear for borrowing until past particular machines or technology and does n't develop panic much or many manufacturers or control\n",
      " very good is a new company hopes that the right reason more high for the market is expected so long\n"
     ]
    }
   ],
   "source": [
    "model.load_params('Rnnlm.pkl')\n",
    "\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[word] for word in skip_words]\n",
    "\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '\\n')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37199fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterRnnlmGen(BetterRnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
    "        word_ids = [start_id]\n",
    "\n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1)\n",
    "            score = self.predict(x).flatten()\n",
    "            p = softmax(score).flatten()\n",
    "\n",
    "            sampled = np.random.choice(len(p), size=1, p=p)\n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))\n",
    "\n",
    "        return word_ids\n",
    "\n",
    "    def get_state(self):\n",
    "        states = []\n",
    "        for layer in self.lstm_layers:\n",
    "            states.append((layer.h, layer.c))\n",
    "        return states\n",
    "\n",
    "    def set_state(self, states):\n",
    "        for layer, state in zip(self.lstm_layers, states):\n",
    "            layer.set_state(*state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a407fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import sequence\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt', \n",
    "                                                         seed=1984)\n",
    "char_to_id, id_to_char = sequence.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cce35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.time_layers import *\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_w = (rn(V, D) / 100).astype('f')\n",
    "        lstm_wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "        lstm_wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4*H).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_w)\n",
    "        self.lstm = TimeLSTM(lstm_wx, lstm_wh, lstm_b, stateful=False)\n",
    "        \n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return hs[:, -1, :]\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:, -1, :] = dh\n",
    "        \n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9779d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
