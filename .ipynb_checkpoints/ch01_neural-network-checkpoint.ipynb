{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba133bc",
   "metadata": {},
   "source": [
    "## forward net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2822bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7b3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, w, b):\n",
    "        self.params = [w, b]\n",
    "    def forward(self, x):\n",
    "        w, b = self.params\n",
    "        return np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33695b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        w1 = np.random.randn(input_size, hidden_size)\n",
    "        b1 = np.random.randn(hidden_size)\n",
    "        w2 = np.random.randn(hidden_size, output_size)\n",
    "        b2 = np.random.randn(output_size)\n",
    "        \n",
    "        self.layers = [\n",
    "            Affine(w1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(w2, b2)\n",
    "        ]\n",
    "        \n",
    "        self.params = [layer.params for layer in self.layers]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faea2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e8ad8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09357391, -0.66419211,  0.58977063],\n",
       "       [ 0.28464052, -0.69942975,  0.38697996],\n",
       "       [ 0.12399453, -0.651302  ,  0.51744641],\n",
       "       [-0.19966585, -0.66022445,  0.65018365],\n",
       "       [-0.67957432, -0.5596603 ,  0.9606917 ],\n",
       "       [-0.13409821, -0.65901281,  0.64366764],\n",
       "       [ 0.18858254, -0.72618797,  0.37571508],\n",
       "       [ 0.791689  , -0.92448846,  0.42585181],\n",
       "       [ 0.62703465, -0.97138159,  0.30494568],\n",
       "       [ 0.58901719, -0.98320963,  0.28357477]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6601e",
   "metadata": {},
   "source": [
    "## backward net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4a060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, w):\n",
    "        self.params = [w]\n",
    "        self.grads = [np.zeros_like(w)]\n",
    "    def forward(self, x):\n",
    "        w, = self.params\n",
    "        self.x = x\n",
    "        return np.matmul(x, w)\n",
    "    def backward(self, dout):\n",
    "        w, = self.params\n",
    "        dx = np.matmul(dout, w.T)\n",
    "        dw = np.matmul(self.x.T, dout)\n",
    "        self.grads[0][...] = dw\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962eaea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "    def forward(self, x):\n",
    "        y = 1 / (1 + np.exp(-x))\n",
    "        self.y = y\n",
    "        return y\n",
    "    def backward(self, dout):\n",
    "        return dout * self.y*(1-self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf37b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, w, b):\n",
    "        self.params = [w, b]\n",
    "        self.grads = [np.zeros_like(w), np.zeros_like(b)]\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        w, b = self.params\n",
    "        return np.matmul(x, w) + b\n",
    "    def backward(self, dout):\n",
    "        w, b = self.params\n",
    "        try:\n",
    "            dx = np.matmul(dout, w.T)\n",
    "            dw = np.matmul(self.x.T, dout)\n",
    "            db = np.sum(dout, axis=0)\n",
    "        except:\n",
    "            print(dout.shape, w.shape)\n",
    "            raise Exception('error')\n",
    "            \n",
    "        self.grads[0][...] = dw\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8df72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "    def softmax(self, x):\n",
    "        exp_ = np.exp(x)\n",
    "        return exp_ / np.sum(exp_, axis=-1, keepdims=True)\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        y = self.softmax(x)\n",
    "        self.y = y\n",
    "        L = -np.sum(t*np.log(y))/len(t)\n",
    "        return L\n",
    "    def backward(self):\n",
    "        return (self.y - self.t) / len(self.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a7fa6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.w1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.w2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        self.layers = [\n",
    "            Affine(self.w1, self.b1),\n",
    "            Sigmoid(),\n",
    "            Affine(self.w2, self.b2)\n",
    "        ]\n",
    "        \n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += [layer.params]\n",
    "            self.grads += [layer.grads]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    def forward(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        loss = self.loss_layer.forward(y, t)\n",
    "        return loss\n",
    "    def backward(self):\n",
    "        dout = self.loss_layer.backward()\n",
    "        for layer in self.layers[::-1]:\n",
    "            dout = layer.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dba23cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c689407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, params, grads):\n",
    "        for param, grad in zip(params, grads):\n",
    "            param -= self.lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088f0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seed=1984):\n",
    "    np.random.seed(seed)\n",
    "    N = 100  # 클래스당 샘플 수\n",
    "    DIM = 2  # 데어터 요소 수\n",
    "    CLS_NUM = 3  # 클래스 수\n",
    "\n",
    "    x = np.zeros((N*CLS_NUM, DIM))\n",
    "    t = np.zeros((N*CLS_NUM, CLS_NUM), dtype=np.int)\n",
    "\n",
    "    for j in range(CLS_NUM):\n",
    "        for i in range(N): # N*j, N*(j+1)):\n",
    "            rate = i / N\n",
    "            radius = 1.0*rate\n",
    "            theta = j*4.0 + 4.0*rate + np.random.randn()*0.2\n",
    "\n",
    "            ix = N*j + i\n",
    "            x[ix] = np.array([radius*np.sin(theta),\n",
    "                              radius*np.cos(theta)]).flatten()\n",
    "            t[ix, j] = 1\n",
    "\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64b2163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinjy\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 1.4541987747234213\n",
      "LOSS: 1.0203866820978795\n",
      "LOSS: 0.8830514981677592\n",
      "LOSS: 0.8176293077031825\n",
      "LOSS: 0.7883858248926694\n",
      "LOSS: 0.7832440710426839\n",
      "LOSS: 0.786459516637784\n",
      "LOSS: 0.7457602547153473\n",
      "LOSS: 0.743739416217815\n",
      "LOSS: 0.7589314573781266\n",
      "LOSS: 0.7515740744657932\n",
      "LOSS: 0.7556641422688253\n",
      "LOSS: 0.756564884525499\n",
      "LOSS: 0.7568418602265526\n",
      "LOSS: 0.7415338984727666\n",
      "LOSS: 0.7853080962871937\n",
      "LOSS: 0.7158341369369061\n",
      "LOSS: 0.715094526437474\n",
      "LOSS: 0.750217231752861\n",
      "LOSS: 0.7426519668141325\n",
      "LOSS: 0.7248709850526023\n",
      "LOSS: 0.7177554915221752\n",
      "LOSS: 0.7048723879962105\n",
      "LOSS: 0.7011046019446989\n",
      "LOSS: 0.7461949959578732\n",
      "LOSS: 0.7069374350831403\n",
      "LOSS: 0.6879220131659194\n",
      "LOSS: 0.6956283311064885\n",
      "LOSS: 0.7092942043652217\n",
      "LOSS: 0.6749735910310832\n",
      "LOSS: 0.6959395575576147\n",
      "LOSS: 0.6890481839948066\n",
      "LOSS: 0.6887545436468494\n",
      "LOSS: 0.6944587666793863\n",
      "LOSS: 0.6572235456817233\n",
      "LOSS: 0.6522929573530418\n",
      "LOSS: 0.6519536480177388\n",
      "LOSS: 0.6513575991873227\n",
      "LOSS: 0.6211827474903173\n",
      "LOSS: 0.6346307665758119\n",
      "LOSS: 0.6343455531435765\n",
      "LOSS: 0.631379219399746\n",
      "LOSS: 0.6269881393134674\n",
      "LOSS: 0.6029021109053623\n",
      "LOSS: 0.6079156966356412\n",
      "LOSS: 0.5802824618283637\n",
      "LOSS: 0.5728482809118332\n",
      "LOSS: 0.5613435632726186\n",
      "LOSS: 0.5572689868439972\n",
      "LOSS: 0.5497193395462779\n",
      "LOSS: 0.538222416968704\n",
      "LOSS: 0.5375452766199105\n",
      "LOSS: 0.5572775646108239\n",
      "LOSS: 0.5311607525862907\n",
      "LOSS: 0.5245059665921274\n",
      "LOSS: 0.4998897655988153\n",
      "LOSS: 0.4845397667006982\n",
      "LOSS: 0.4735140611701656\n",
      "LOSS: 0.48800351778736095\n",
      "LOSS: 0.4603815635874362\n",
      "LOSS: 0.46107251159481677\n",
      "LOSS: 0.4569204474137476\n",
      "LOSS: 0.4541954852531832\n",
      "LOSS: 0.46843241630720467\n",
      "LOSS: 0.4458828819155407\n",
      "LOSS: 0.4187039120190724\n",
      "LOSS: 0.43739111329587954\n",
      "LOSS: 0.40379253391562686\n",
      "LOSS: 0.4056670793492433\n",
      "LOSS: 0.4122344173944909\n",
      "LOSS: 0.3944777301454335\n",
      "LOSS: 0.3791288084465657\n",
      "LOSS: 0.36971975454479894\n",
      "LOSS: 0.3777603936557695\n",
      "LOSS: 0.3647320609989725\n",
      "LOSS: 0.3469881712707619\n",
      "LOSS: 0.37561707668318534\n",
      "LOSS: 0.3494518763709339\n",
      "LOSS: 0.3459383868505469\n",
      "LOSS: 0.32553784783180933\n",
      "LOSS: 0.3235581607121253\n",
      "LOSS: 0.35694616307373417\n",
      "LOSS: 0.3183283075806501\n",
      "LOSS: 0.31893322408829383\n",
      "LOSS: 0.3208093272378433\n",
      "LOSS: 0.3053430292892739\n",
      "LOSS: 0.29624062025470965\n",
      "LOSS: 0.3156176670980929\n",
      "LOSS: 0.3000290551909791\n",
      "LOSS: 0.2939317974329582\n",
      "LOSS: 0.2880854051090792\n",
      "LOSS: 0.2805019741590479\n",
      "LOSS: 0.285435338334303\n",
      "LOSS: 0.2707587556636245\n",
      "LOSS: 0.27373642846297863\n",
      "LOSS: 0.26627336749890146\n",
      "LOSS: 0.2651242551016494\n",
      "LOSS: 0.2744793600843091\n",
      "LOSS: 0.26296080299916125\n",
      "LOSS: 0.2654909051518408\n",
      "LOSS: 0.25923766696907696\n",
      "LOSS: 0.2524601433881339\n",
      "LOSS: 0.2617211461598748\n",
      "LOSS: 0.2564334494602002\n",
      "LOSS: 0.2619303330557866\n",
      "LOSS: 0.23581142359407964\n",
      "LOSS: 0.2376036269200176\n",
      "LOSS: 0.24865410842542324\n",
      "LOSS: 0.2527590617484758\n",
      "LOSS: 0.25662646431427555\n",
      "LOSS: 0.24231608989453543\n",
      "LOSS: 0.24023303665545703\n",
      "LOSS: 0.22901437287573914\n",
      "LOSS: 0.24648738788833963\n",
      "LOSS: 0.22197023172085176\n",
      "LOSS: 0.22246470737889243\n",
      "LOSS: 0.21355316939475996\n",
      "LOSS: 0.2248021093427277\n",
      "LOSS: 0.21767883212832836\n",
      "LOSS: 0.2442527632986884\n",
      "LOSS: 0.2077230788646085\n",
      "LOSS: 0.22185826149848556\n",
      "LOSS: 0.21077724854092947\n",
      "LOSS: 0.2209981592759275\n",
      "LOSS: 0.2191780681200714\n",
      "LOSS: 0.2112257543658102\n",
      "LOSS: 0.19815780441949635\n",
      "LOSS: 0.19456981639428597\n",
      "LOSS: 0.20439555499508652\n",
      "LOSS: 0.19665444915259148\n",
      "LOSS: 0.19706101324603315\n",
      "LOSS: 0.19266142363612565\n",
      "LOSS: 0.2047431976455205\n",
      "LOSS: 0.2058344768135913\n",
      "LOSS: 0.20283914508171227\n",
      "LOSS: 0.2078582794264722\n",
      "LOSS: 0.19872545459229998\n",
      "LOSS: 0.19692541185569318\n",
      "LOSS: 0.19408456403099342\n",
      "LOSS: 0.186771087473317\n",
      "LOSS: 0.18078609774029436\n",
      "LOSS: 0.19647747735973053\n",
      "LOSS: 0.18550117063985425\n",
      "LOSS: 0.1937209565222358\n",
      "LOSS: 0.18517073662907302\n",
      "LOSS: 0.18546796873438204\n",
      "LOSS: 0.18282569926473208\n",
      "LOSS: 0.18225841137288956\n",
      "LOSS: 0.1742909198113266\n",
      "LOSS: 0.17899931378403536\n",
      "LOSS: 0.17980430466533145\n",
      "LOSS: 0.18050953998092173\n",
      "LOSS: 0.17600439333729617\n",
      "LOSS: 0.17781363092339147\n",
      "LOSS: 0.17507235269066615\n",
      "LOSS: 0.1714449186428675\n",
      "LOSS: 0.17019875827908404\n",
      "LOSS: 0.17210963810855956\n",
      "LOSS: 0.174142091278286\n",
      "LOSS: 0.1652837011960039\n",
      "LOSS: 0.17427850577694082\n",
      "LOSS: 0.16359205895175022\n",
      "LOSS: 0.18618978075432416\n",
      "LOSS: 0.1650836968502662\n",
      "LOSS: 0.16615381493585643\n",
      "LOSS: 0.16623704777335246\n",
      "LOSS: 0.1572002221546695\n",
      "LOSS: 0.15765007536158462\n",
      "LOSS: 0.15905380549090792\n",
      "LOSS: 0.15913258115575524\n",
      "LOSS: 0.16373233270536253\n",
      "LOSS: 0.15237878934771684\n",
      "LOSS: 0.1560208637137214\n",
      "LOSS: 0.15664958977558566\n",
      "LOSS: 0.15536258880384318\n",
      "LOSS: 0.148998110357799\n",
      "LOSS: 0.15213029606167675\n",
      "LOSS: 0.15380363972911845\n",
      "LOSS: 0.14995195813308204\n",
      "LOSS: 0.15097980107713843\n",
      "LOSS: 0.15758585081234538\n",
      "LOSS: 0.14907162451101724\n",
      "LOSS: 0.15965944458727738\n",
      "LOSS: 0.15666789149035382\n",
      "LOSS: 0.1497697649433716\n",
      "LOSS: 0.15893565580884933\n",
      "LOSS: 0.15367829280398468\n",
      "LOSS: 0.14889629900989002\n",
      "LOSS: 0.14913046460440832\n",
      "LOSS: 0.15177491270190793\n",
      "LOSS: 0.14359774021040195\n",
      "LOSS: 0.15479448360265122\n",
      "LOSS: 0.14653758648666595\n",
      "LOSS: 0.13697604810900382\n",
      "LOSS: 0.14903624744365168\n",
      "LOSS: 0.1478971380141108\n",
      "LOSS: 0.14863874944833508\n",
      "LOSS: 0.13344629197854385\n",
      "LOSS: 0.13715309336375206\n",
      "LOSS: 0.1476550430363016\n",
      "LOSS: 0.14147450955539032\n",
      "LOSS: 0.15208738346507772\n",
      "LOSS: 0.13988170870129985\n",
      "LOSS: 0.14296744263625788\n",
      "LOSS: 0.1455356037121267\n",
      "LOSS: 0.14079910882035301\n",
      "LOSS: 0.14039192319197497\n",
      "LOSS: 0.1364130950253542\n",
      "LOSS: 0.133953164542091\n",
      "LOSS: 0.13536918732966113\n",
      "LOSS: 0.13729133270391472\n",
      "LOSS: 0.13806616163123936\n",
      "LOSS: 0.1342181802703296\n",
      "LOSS: 0.13269896096746042\n",
      "LOSS: 0.14481521781379383\n",
      "LOSS: 0.13122062219021563\n",
      "LOSS: 0.13156269345668795\n",
      "LOSS: 0.13710153554650212\n",
      "LOSS: 0.1314766954071375\n",
      "LOSS: 0.1329127266388349\n",
      "LOSS: 0.1346082564117364\n",
      "LOSS: 0.13298222255025932\n",
      "LOSS: 0.13480428843764003\n",
      "LOSS: 0.1403429828323037\n",
      "LOSS: 0.12968998841364152\n",
      "LOSS: 0.13152909701375703\n",
      "LOSS: 0.13381619491420554\n",
      "LOSS: 0.12718511084723075\n",
      "LOSS: 0.11950490309129003\n",
      "LOSS: 0.12750512559978924\n",
      "LOSS: 0.12288001203608719\n",
      "LOSS: 0.1284072555844119\n",
      "LOSS: 0.12261153631955173\n",
      "LOSS: 0.12288889819321538\n",
      "LOSS: 0.12530453963083507\n",
      "LOSS: 0.12299165136213663\n",
      "LOSS: 0.12815276165580589\n",
      "LOSS: 0.12795964306479354\n",
      "LOSS: 0.12553993629890398\n",
      "LOSS: 0.137454766325585\n",
      "LOSS: 0.12755501597480554\n",
      "LOSS: 0.12606878222871895\n",
      "LOSS: 0.12217529256092914\n",
      "LOSS: 0.1274924049589591\n",
      "LOSS: 0.12529800974226168\n",
      "LOSS: 0.12165182156283152\n",
      "LOSS: 0.11892522903243671\n",
      "LOSS: 0.13682155282903607\n",
      "LOSS: 0.12287852938477692\n",
      "LOSS: 0.12446595899426302\n",
      "LOSS: 0.13355481608530445\n",
      "LOSS: 0.11700012802870727\n",
      "LOSS: 0.11349022695963754\n",
      "LOSS: 0.11623372451124278\n",
      "LOSS: 0.11773622031121012\n",
      "LOSS: 0.1198856214259281\n",
      "LOSS: 0.11486324411094026\n",
      "LOSS: 0.11950027245279123\n",
      "LOSS: 0.12958207484244358\n",
      "LOSS: 0.11473899244043514\n",
      "LOSS: 0.12354908686108435\n",
      "LOSS: 0.12074911534790148\n",
      "LOSS: 0.11323101550426182\n",
      "LOSS: 0.13081204188998755\n",
      "LOSS: 0.11284236859241\n",
      "LOSS: 0.11946912483230618\n",
      "LOSS: 0.1189897354771156\n",
      "LOSS: 0.11315478661382364\n",
      "LOSS: 0.11082519672785726\n",
      "LOSS: 0.11801004777553495\n",
      "LOSS: 0.11075461184715478\n",
      "LOSS: 0.11577363674000948\n",
      "LOSS: 0.11321120563526903\n",
      "LOSS: 0.11877612174331476\n",
      "LOSS: 0.11218190842628273\n",
      "LOSS: 0.11214112631666648\n",
      "LOSS: 0.12423792532857669\n",
      "LOSS: 0.1106345409098906\n",
      "LOSS: 0.11098954763450113\n",
      "LOSS: 0.11108053742229758\n",
      "LOSS: 0.10831421231496227\n",
      "LOSS: 0.11664283144977812\n",
      "LOSS: 0.1104215978109386\n",
      "LOSS: 0.11360225435928979\n",
      "LOSS: 0.11335890161567104\n",
      "LOSS: 0.10733451697824954\n",
      "LOSS: 0.11174494056823663\n",
      "LOSS: 0.11744727968096313\n",
      "LOSS: 0.11367083209279709\n",
      "LOSS: 0.11882585723212025\n",
      "LOSS: 0.1127018631978944\n",
      "LOSS: 0.11225762970639212\n",
      "LOSS: 0.11187814049956964\n",
      "LOSS: 0.11323225725045354\n",
      "LOSS: 0.11962728944454364\n",
      "LOSS: 0.10957929146819859\n",
      "LOSS: 0.1224597701682169\n",
      "LOSS: 0.11400432361622559\n",
      "LOSS: 0.10381030018276302\n",
      "LOSS: 0.10769164916061129\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "x, t = load_data()\n",
    "D_len = len(t)\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "max_iter = D_len//batch_size\n",
    "\n",
    "for epoch in range(1, max_epoch+1):\n",
    "    idx_lst = np.random.permutation(D_len)\n",
    "    x, t = x[idx_lst], t[idx_lst]\n",
    "    loss_total = 0\n",
    "    for iter in range(max_iter):    \n",
    "        x_batch, t_batch = x[iter*batch_size: (iter+1)*batch_size], t[iter*batch_size: (iter+1)*batch_size]\n",
    "        loss = model.forward(x_batch, t_batch)\n",
    "        model.backward()\n",
    "        for param, grad in zip(model.params, model.grads):\n",
    "            optimizer.update(param, grad)\n",
    "        loss_total += loss\n",
    "    print('LOSS: {}'.format(loss_total/max_iter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
