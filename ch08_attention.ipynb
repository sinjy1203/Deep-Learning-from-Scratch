{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c78899f",
   "metadata": {},
   "source": [
    "### 전 RNN은 encoder의 아웃풋이 단어길이와 상관없이 고정길이 (N X T)\n",
    "### ==> 정보표현의 한계\n",
    "\n",
    "### Attention: encoder rnn의 모든 은닉상태벡터 사용\n",
    "### ==> time에 따른 정보표현 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ccdd2c",
   "metadata": {},
   "source": [
    "### aliment: input에서 필요한 정보만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc65d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N, T, 1)\n",
    "        t = hs * ar\n",
    "        \n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05576084",
   "metadata": {},
   "source": [
    "### encoder의 은닉상태와 decoder의 은닉상태의 유사도를 가중치로 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7bb520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        hr = h.reshape(N, 1, H)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "        \n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c61b44",
   "metadata": {},
   "source": [
    "### AttentionWeight: encoder의 hs와 decoder의 h의 유사도(중요도 가중치)\n",
    "\n",
    "### WeightSum: hs의 a를 통한 가중치합\n",
    "\n",
    "### Attention: AttentionWeight + WeightSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a03ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ac3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "        \n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32bb037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.time_layers import *\n",
    "from common.other_class import Encoder, Seq2seq\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a175a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f') # attention + lstm\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "815cd956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208cea0",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a44e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2024df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_char[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c05017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99401975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 7[s] | 손실 3.10\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 15[s] | 손실 1.89\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 22[s] | 손실 1.73\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 30[s] | 손실 1.53\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 38[s] | 손실 1.22\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 45[s] | 손실 1.16\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 54[s] | 손실 1.11\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 62[s] | 손실 1.07\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 70[s] | 손실 1.05\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 78[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 86[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 94[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 103[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 111[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 119[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 127[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 135[s] | 손실 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1993-03-21\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1993-03-21\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1993-03-24\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1993-03-21\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1993-03-21\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1993-03-21\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1993-03-21\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1993-03-24\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1993-03-21\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1993-03-21\n",
      "---\n",
      "acc 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 12[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 22[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 30[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 37[s] | 손실 0.96\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 45[s] | 손실 0.95\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 53[s] | 손실 0.92\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 62[s] | 손실 0.89\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 70[s] | 손실 0.87\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 79[s] | 손실 0.83\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 88[s] | 손실 0.80\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 98[s] | 손실 0.77\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 105[s] | 손실 0.72\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 113[s] | 손실 0.67\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 122[s] | 손실 0.63\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 131[s] | 손실 0.58\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 143[s] | 손실 0.55\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 156[s] | 손실 0.51\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1994-12-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2003-05-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 2014-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1977-07-19\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 2008-09-29\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2000-09-04\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 2011-12-27\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2014-11-06\n",
      "---\n",
      "acc 12.200%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 9[s] | 손실 0.45\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 17[s] | 손실 0.41\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 25[s] | 손실 0.37\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 33[s] | 손실 0.32\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 41[s] | 손실 0.28\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 50[s] | 손실 0.23\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 57[s] | 손실 0.19\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 65[s] | 손실 0.15\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 73[s] | 손실 0.13\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 81[s] | 손실 0.12\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 89[s] | 손실 0.10\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 97[s] | 손실 0.08\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 105[s] | 손실 0.07\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 113[s] | 손실 0.06\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 121[s] | 손실 0.05\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 129[s] | 손실 0.04\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 137[s] | 손실 0.04\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "acc 96.120%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.03\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 7[s] | 손실 0.03\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 15[s] | 손실 0.03\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 23[s] | 손실 0.02\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 30[s] | 손실 0.02\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 38[s] | 손실 0.02\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 45[s] | 손실 0.02\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 52[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 60[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 67[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 74[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 82[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 89[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 97[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 104[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 111[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 119[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 126[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "acc 99.920%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 7[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 15[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 22[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 29[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 37[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 44[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 59[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 74[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 81[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 89[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 96[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 119[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 126[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "acc 100.000%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 7[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 15[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 22[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 29[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 36[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 44[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 51[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 59[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 66[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 81[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 96[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 103[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 111[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 119[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 126[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 100.000%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 7[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 15[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 30[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 38[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 45[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 60[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 75[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 82[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 90[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 105[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 120[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 127[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "acc 100.000%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 7[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 15[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 22[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 30[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 37[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 45[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 60[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 75[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 82[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 90[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 105[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 120[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 127[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "acc 100.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 7[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 15[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 22[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 29[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 37[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 44[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 59[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 74[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 82[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 89[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 119[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 127[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "acc 100.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 7[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 15[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 22[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 30[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 38[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 45[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 60[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 75[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 82[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 90[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 105[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 120[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 127[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "acc 100.000%\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size,\n",
    "               max_grad=max_grad)\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct, \n",
    "                                   id_to_char, verbose, is_reverse=True)\n",
    "    \n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('acc %.3f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99e78961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
