{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba133bc",
   "metadata": {},
   "source": [
    "## forward net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2822bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7b3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, w, b):\n",
    "        self.params = [w, b]\n",
    "    def forward(self, x):\n",
    "        w, b = self.params\n",
    "        return np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33695b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        w1 = np.random.randn(input_size, hidden_size)\n",
    "        b1 = np.random.randn(hidden_size)\n",
    "        w2 = np.random.randn(hidden_size, output_size)\n",
    "        b2 = np.random.randn(output_size)\n",
    "        \n",
    "        self.layers = [\n",
    "            Affine(w1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(w2, b2)\n",
    "        ]\n",
    "        \n",
    "        self.params = [layer.params for layer in self.layers]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faea2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e8ad8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.36659756,  1.44213757,  0.99868739],\n",
       "       [-0.41108669,  0.70334962,  0.89103937],\n",
       "       [-0.61282534,  0.04465246,  1.67300194],\n",
       "       [-0.29463998,  0.41696675,  1.10793807],\n",
       "       [-0.81306355,  1.53688514,  0.4100552 ],\n",
       "       [-0.67412339,  0.45468624,  1.34052626],\n",
       "       [-1.16229462,  0.94312305,  1.29231276],\n",
       "       [-0.90508016,  1.42409483,  0.60200373],\n",
       "       [-1.09811362,  2.31741691, -0.09913869],\n",
       "       [-1.50299399,  1.78321892,  0.79056318]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6601e",
   "metadata": {},
   "source": [
    "## backward net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea4a060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, w):\n",
    "        self.params = [w]\n",
    "        self.grads = [np.zeros_like(w)]\n",
    "    def forward(self, x):\n",
    "        w, = self.params\n",
    "        self.x = x\n",
    "        return np.matmul(x, w)\n",
    "    def backward(self, dout):\n",
    "        w, = self.params\n",
    "        dx = np.matmul(dout, w.T)\n",
    "        dw = np.matmul(self.x.T, dout)\n",
    "        self.grads[0][...] = dw\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962eaea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "    def forward(self, x):\n",
    "        y = 1 / (1 + np.exp(-x))\n",
    "        self.y = y\n",
    "        return y\n",
    "    def backward(self, dout):\n",
    "        return dout * self.y*(1-self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf37b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, w, b):\n",
    "        self.params = [w, b]\n",
    "        self.grads = [np.zeros_like(w), np.zeros_like(b)]\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        w, b = self.params\n",
    "        return np.matmul(x, w) + b\n",
    "    def backward(self, dout):\n",
    "        w, b = self.params\n",
    "        try:\n",
    "            dx = np.matmul(dout, w.T)\n",
    "            dw = np.matmul(self.x.T, dout)\n",
    "            db = np.sum(dout, axis=0)\n",
    "        except:\n",
    "            print(dout.shape, w.shape)\n",
    "            raise Exception('error')\n",
    "            \n",
    "        self.grads[0][...] = dw\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8df72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "    def softmax(self, x):\n",
    "        exp_ = np.exp(x)\n",
    "        return exp_ / np.sum(exp_, axis=-1, keepdims=True)\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        y = self.softmax(x)\n",
    "        self.y = y\n",
    "        L = -np.sum(t*np.log(y))/len(t)\n",
    "        return L\n",
    "    def backward(self):\n",
    "        return (self.y - self.t) / len(self.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a7fa6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.w1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.w2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        self.layers = [\n",
    "            Affine(self.w1, self.b1),\n",
    "            Sigmoid(),\n",
    "            Affine(self.w2, self.b2)\n",
    "        ]\n",
    "        \n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += [layer.params]\n",
    "            self.grads += [layer.grads]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    def forward(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        loss = self.loss_layer.forward(y, t)\n",
    "        return loss\n",
    "    def backward(self):\n",
    "        dout = self.loss_layer.backward()\n",
    "        for layer in self.layers[::-1]:\n",
    "            dout = layer.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c689407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, params, grads):\n",
    "        for param, grad in zip(params, grads):\n",
    "            param -= self.lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "088f0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seed=1984):\n",
    "    np.random.seed(seed)\n",
    "    N = 100  # 클래스당 샘플 수\n",
    "    DIM = 2  # 데어터 요소 수\n",
    "    CLS_NUM = 3  # 클래스 수\n",
    "\n",
    "    x = np.zeros((N*CLS_NUM, DIM))\n",
    "    t = np.zeros((N*CLS_NUM, CLS_NUM), dtype=np.int)\n",
    "\n",
    "    for j in range(CLS_NUM):\n",
    "        for i in range(N): # N*j, N*(j+1)):\n",
    "            rate = i / N\n",
    "            radius = 1.0*rate\n",
    "            theta = j*4.0 + 4.0*rate + np.random.randn()*0.2\n",
    "\n",
    "            ix = N*j + i\n",
    "            x[ix] = np.array([radius*np.sin(theta),\n",
    "                              radius*np.cos(theta)]).flatten()\n",
    "            t[ix, j] = 1\n",
    "\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b2163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinjy\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 1.4541987747234213\n",
      "LOSS: 1.0203866820978795\n",
      "LOSS: 0.8830514981677592\n",
      "LOSS: 0.8176293077031825\n",
      "LOSS: 0.7883858248926694\n",
      "LOSS: 0.7832440710426839\n",
      "LOSS: 0.786459516637784\n",
      "LOSS: 0.7457602547153473\n",
      "LOSS: 0.743739416217815\n",
      "LOSS: 0.7589314573781266\n",
      "LOSS: 0.7515740744657932\n",
      "LOSS: 0.7556641422688253\n",
      "LOSS: 0.756564884525499\n",
      "LOSS: 0.7568418602265526\n",
      "LOSS: 0.7415338984727666\n",
      "LOSS: 0.7853080962871937\n",
      "LOSS: 0.7158341369369061\n",
      "LOSS: 0.715094526437474\n",
      "LOSS: 0.750217231752861\n",
      "LOSS: 0.7426519668141325\n",
      "LOSS: 0.7248709850526023\n",
      "LOSS: 0.7177554915221752\n",
      "LOSS: 0.7048723879962105\n",
      "LOSS: 0.7011046019446989\n",
      "LOSS: 0.7461949959578732\n",
      "LOSS: 0.7069374350831403\n",
      "LOSS: 0.6879220131659194\n",
      "LOSS: 0.6956283311064885\n",
      "LOSS: 0.7092942043652217\n",
      "LOSS: 0.6749735910310832\n",
      "LOSS: 0.6959395575576147\n",
      "LOSS: 0.6890481839948066\n",
      "LOSS: 0.6887545436468494\n",
      "LOSS: 0.6944587666793863\n",
      "LOSS: 0.6572235456817233\n",
      "LOSS: 0.6522929573530418\n",
      "LOSS: 0.6519536480177388\n",
      "LOSS: 0.6513575991873227\n",
      "LOSS: 0.6211827474903173\n",
      "LOSS: 0.6346307665758119\n",
      "LOSS: 0.6343455531435765\n",
      "LOSS: 0.631379219399746\n",
      "LOSS: 0.6269881393134674\n",
      "LOSS: 0.6029021109053623\n",
      "LOSS: 0.6079156966356412\n",
      "LOSS: 0.5802824618283637\n",
      "LOSS: 0.5728482809118332\n",
      "LOSS: 0.5613435632726186\n",
      "LOSS: 0.5572689868439972\n",
      "LOSS: 0.5497193395462779\n",
      "LOSS: 0.538222416968704\n",
      "LOSS: 0.5375452766199105\n",
      "LOSS: 0.5572775646108239\n",
      "LOSS: 0.5311607525862907\n",
      "LOSS: 0.5245059665921274\n",
      "LOSS: 0.4998897655988153\n",
      "LOSS: 0.4845397667006982\n",
      "LOSS: 0.4735140611701656\n",
      "LOSS: 0.48800351778736095\n",
      "LOSS: 0.4603815635874362\n",
      "LOSS: 0.46107251159481677\n",
      "LOSS: 0.4569204474137476\n",
      "LOSS: 0.4541954852531832\n",
      "LOSS: 0.46843241630720467\n",
      "LOSS: 0.4458828819155407\n",
      "LOSS: 0.4187039120190724\n",
      "LOSS: 0.43739111329587954\n",
      "LOSS: 0.40379253391562686\n",
      "LOSS: 0.4056670793492433\n",
      "LOSS: 0.4122344173944909\n",
      "LOSS: 0.3944777301454335\n",
      "LOSS: 0.3791288084465657\n",
      "LOSS: 0.36971975454479894\n",
      "LOSS: 0.3777603936557695\n",
      "LOSS: 0.3647320609989725\n",
      "LOSS: 0.3469881712707619\n",
      "LOSS: 0.37561707668318534\n",
      "LOSS: 0.3494518763709339\n",
      "LOSS: 0.3459383868505469\n",
      "LOSS: 0.32553784783180933\n",
      "LOSS: 0.3235581607121253\n",
      "LOSS: 0.35694616307373417\n",
      "LOSS: 0.3183283075806501\n",
      "LOSS: 0.31893322408829383\n",
      "LOSS: 0.3208093272378433\n",
      "LOSS: 0.3053430292892739\n",
      "LOSS: 0.29624062025470965\n",
      "LOSS: 0.3156176670980929\n",
      "LOSS: 0.3000290551909791\n",
      "LOSS: 0.2939317974329582\n",
      "LOSS: 0.2880854051090792\n",
      "LOSS: 0.2805019741590479\n",
      "LOSS: 0.285435338334303\n",
      "LOSS: 0.2707587556636245\n",
      "LOSS: 0.27373642846297863\n",
      "LOSS: 0.26627336749890146\n",
      "LOSS: 0.2651242551016494\n",
      "LOSS: 0.2744793600843091\n",
      "LOSS: 0.26296080299916125\n",
      "LOSS: 0.2654909051518408\n",
      "LOSS: 0.25923766696907696\n",
      "LOSS: 0.2524601433881339\n",
      "LOSS: 0.2617211461598748\n",
      "LOSS: 0.2564334494602002\n",
      "LOSS: 0.2619303330557866\n",
      "LOSS: 0.23581142359407964\n",
      "LOSS: 0.2376036269200176\n",
      "LOSS: 0.24865410842542324\n",
      "LOSS: 0.2527590617484758\n",
      "LOSS: 0.25662646431427555\n",
      "LOSS: 0.24231608989453543\n",
      "LOSS: 0.24023303665545703\n",
      "LOSS: 0.22901437287573914\n",
      "LOSS: 0.24648738788833963\n",
      "LOSS: 0.22197023172085176\n",
      "LOSS: 0.22246470737889243\n",
      "LOSS: 0.21355316939475996\n",
      "LOSS: 0.2248021093427277\n",
      "LOSS: 0.21767883212832836\n",
      "LOSS: 0.2442527632986884\n",
      "LOSS: 0.2077230788646085\n",
      "LOSS: 0.22185826149848556\n",
      "LOSS: 0.21077724854092947\n",
      "LOSS: 0.2209981592759275\n",
      "LOSS: 0.2191780681200714\n",
      "LOSS: 0.2112257543658102\n",
      "LOSS: 0.19815780441949635\n",
      "LOSS: 0.19456981639428597\n",
      "LOSS: 0.20439555499508652\n",
      "LOSS: 0.19665444915259148\n",
      "LOSS: 0.19706101324603315\n",
      "LOSS: 0.19266142363612565\n",
      "LOSS: 0.2047431976455205\n",
      "LOSS: 0.2058344768135913\n",
      "LOSS: 0.20283914508171227\n",
      "LOSS: 0.2078582794264722\n",
      "LOSS: 0.19872545459229998\n",
      "LOSS: 0.19692541185569318\n",
      "LOSS: 0.19408456403099342\n",
      "LOSS: 0.186771087473317\n",
      "LOSS: 0.18078609774029436\n",
      "LOSS: 0.19647747735973053\n",
      "LOSS: 0.18550117063985425\n",
      "LOSS: 0.1937209565222358\n",
      "LOSS: 0.18517073662907302\n",
      "LOSS: 0.18546796873438204\n",
      "LOSS: 0.18282569926473208\n",
      "LOSS: 0.18225841137288956\n",
      "LOSS: 0.1742909198113266\n",
      "LOSS: 0.17899931378403536\n",
      "LOSS: 0.17980430466533145\n",
      "LOSS: 0.18050953998092173\n",
      "LOSS: 0.17600439333729617\n",
      "LOSS: 0.17781363092339147\n",
      "LOSS: 0.17507235269066615\n",
      "LOSS: 0.1714449186428675\n",
      "LOSS: 0.17019875827908404\n",
      "LOSS: 0.17210963810855956\n",
      "LOSS: 0.174142091278286\n",
      "LOSS: 0.1652837011960039\n",
      "LOSS: 0.17427850577694082\n",
      "LOSS: 0.16359205895175022\n",
      "LOSS: 0.18618978075432416\n",
      "LOSS: 0.1650836968502662\n",
      "LOSS: 0.16615381493585643\n",
      "LOSS: 0.16623704777335246\n",
      "LOSS: 0.1572002221546695\n",
      "LOSS: 0.15765007536158462\n",
      "LOSS: 0.15905380549090792\n",
      "LOSS: 0.15913258115575524\n",
      "LOSS: 0.16373233270536253\n",
      "LOSS: 0.15237878934771684\n",
      "LOSS: 0.1560208637137214\n",
      "LOSS: 0.15664958977558566\n",
      "LOSS: 0.15536258880384318\n",
      "LOSS: 0.148998110357799\n",
      "LOSS: 0.15213029606167675\n",
      "LOSS: 0.15380363972911845\n",
      "LOSS: 0.14995195813308204\n",
      "LOSS: 0.15097980107713843\n",
      "LOSS: 0.15758585081234538\n",
      "LOSS: 0.14907162451101724\n",
      "LOSS: 0.15965944458727738\n",
      "LOSS: 0.15666789149035382\n",
      "LOSS: 0.1497697649433716\n",
      "LOSS: 0.15893565580884933\n",
      "LOSS: 0.15367829280398468\n",
      "LOSS: 0.14889629900989002\n",
      "LOSS: 0.14913046460440832\n",
      "LOSS: 0.15177491270190793\n",
      "LOSS: 0.14359774021040195\n",
      "LOSS: 0.15479448360265122\n",
      "LOSS: 0.14653758648666595\n",
      "LOSS: 0.13697604810900382\n",
      "LOSS: 0.14903624744365168\n",
      "LOSS: 0.1478971380141108\n",
      "LOSS: 0.14863874944833508\n",
      "LOSS: 0.13344629197854385\n",
      "LOSS: 0.13715309336375206\n",
      "LOSS: 0.1476550430363016\n",
      "LOSS: 0.14147450955539032\n",
      "LOSS: 0.15208738346507772\n",
      "LOSS: 0.13988170870129985\n",
      "LOSS: 0.14296744263625788\n",
      "LOSS: 0.1455356037121267\n",
      "LOSS: 0.14079910882035301\n",
      "LOSS: 0.14039192319197497\n",
      "LOSS: 0.1364130950253542\n",
      "LOSS: 0.133953164542091\n",
      "LOSS: 0.13536918732966113\n",
      "LOSS: 0.13729133270391472\n",
      "LOSS: 0.13806616163123936\n",
      "LOSS: 0.1342181802703296\n",
      "LOSS: 0.13269896096746042\n",
      "LOSS: 0.14481521781379383\n",
      "LOSS: 0.13122062219021563\n",
      "LOSS: 0.13156269345668795\n",
      "LOSS: 0.13710153554650212\n",
      "LOSS: 0.1314766954071375\n",
      "LOSS: 0.1329127266388349\n",
      "LOSS: 0.1346082564117364\n",
      "LOSS: 0.13298222255025932\n",
      "LOSS: 0.13480428843764003\n",
      "LOSS: 0.1403429828323037\n",
      "LOSS: 0.12968998841364152\n",
      "LOSS: 0.13152909701375703\n",
      "LOSS: 0.13381619491420554\n",
      "LOSS: 0.12718511084723075\n",
      "LOSS: 0.11950490309129003\n",
      "LOSS: 0.12750512559978924\n",
      "LOSS: 0.12288001203608719\n",
      "LOSS: 0.1284072555844119\n",
      "LOSS: 0.12261153631955173\n",
      "LOSS: 0.12288889819321538\n",
      "LOSS: 0.12530453963083507\n",
      "LOSS: 0.12299165136213663\n",
      "LOSS: 0.12815276165580589\n",
      "LOSS: 0.12795964306479354\n",
      "LOSS: 0.12553993629890398\n",
      "LOSS: 0.137454766325585\n",
      "LOSS: 0.12755501597480554\n",
      "LOSS: 0.12606878222871895\n",
      "LOSS: 0.12217529256092914\n",
      "LOSS: 0.1274924049589591\n",
      "LOSS: 0.12529800974226168\n",
      "LOSS: 0.12165182156283152\n",
      "LOSS: 0.11892522903243671\n",
      "LOSS: 0.13682155282903607\n",
      "LOSS: 0.12287852938477692\n",
      "LOSS: 0.12446595899426302\n",
      "LOSS: 0.13355481608530445\n",
      "LOSS: 0.11700012802870727\n",
      "LOSS: 0.11349022695963754\n",
      "LOSS: 0.11623372451124278\n",
      "LOSS: 0.11773622031121012\n",
      "LOSS: 0.1198856214259281\n",
      "LOSS: 0.11486324411094026\n",
      "LOSS: 0.11950027245279123\n",
      "LOSS: 0.12958207484244358\n",
      "LOSS: 0.11473899244043514\n",
      "LOSS: 0.12354908686108435\n",
      "LOSS: 0.12074911534790148\n",
      "LOSS: 0.11323101550426182\n",
      "LOSS: 0.13081204188998755\n",
      "LOSS: 0.11284236859241\n",
      "LOSS: 0.11946912483230618\n",
      "LOSS: 0.1189897354771156\n",
      "LOSS: 0.11315478661382364\n",
      "LOSS: 0.11082519672785726\n",
      "LOSS: 0.11801004777553495\n",
      "LOSS: 0.11075461184715478\n",
      "LOSS: 0.11577363674000948\n",
      "LOSS: 0.11321120563526903\n",
      "LOSS: 0.11877612174331476\n",
      "LOSS: 0.11218190842628273\n",
      "LOSS: 0.11214112631666648\n",
      "LOSS: 0.12423792532857669\n",
      "LOSS: 0.1106345409098906\n",
      "LOSS: 0.11098954763450113\n",
      "LOSS: 0.11108053742229758\n",
      "LOSS: 0.10831421231496227\n",
      "LOSS: 0.11664283144977812\n",
      "LOSS: 0.1104215978109386\n",
      "LOSS: 0.11360225435928979\n",
      "LOSS: 0.11335890161567104\n",
      "LOSS: 0.10733451697824954\n",
      "LOSS: 0.11174494056823663\n",
      "LOSS: 0.11744727968096313\n",
      "LOSS: 0.11367083209279709\n",
      "LOSS: 0.11882585723212025\n",
      "LOSS: 0.1127018631978944\n",
      "LOSS: 0.11225762970639212\n",
      "LOSS: 0.11187814049956964\n",
      "LOSS: 0.11323225725045354\n",
      "LOSS: 0.11962728944454364\n",
      "LOSS: 0.10957929146819859\n",
      "LOSS: 0.1224597701682169\n",
      "LOSS: 0.11400432361622559\n",
      "LOSS: 0.10381030018276302\n",
      "LOSS: 0.10769164916061129\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "x, t = load_data()\n",
    "D_len = len(t)\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "max_iter = D_len//batch_size\n",
    "\n",
    "for epoch in range(1, max_epoch+1):\n",
    "    idx_lst = np.random.permutation(D_len)\n",
    "    x, t = x[idx_lst], t[idx_lst]\n",
    "    loss_total = 0\n",
    "    for iter in range(max_iter):    \n",
    "        x_batch, t_batch = x[iter*batch_size: (iter+1)*batch_size], t[iter*batch_size: (iter+1)*batch_size]\n",
    "        loss = model.forward(x_batch, t_batch)\n",
    "        model.backward()\n",
    "        for param, grad in zip(model.params, model.grads):\n",
    "            optimizer.update(param, grad)\n",
    "        loss_total += loss\n",
    "    print('LOSS: {}'.format(loss_total/max_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da1ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model, self.optimizer = model, optimizer\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        D_len = len(t)\n",
    "        max_iter = D_len//batch_size\n",
    "        self.loss_lst = []\n",
    "\n",
    "        for epoch in range(1, max_epoch+1):\n",
    "            idx_lst = np.random.permutation(D_len)\n",
    "            x, t = x[idx_lst], t[idx_lst]\n",
    "            loss_total = 0\n",
    "            for iters in range(max_iter):    \n",
    "                x_batch, t_batch = x[iters*batch_size: (iters+1)*batch_size], t[iters*batch_size: (iters+1)*batch_size]\n",
    "                loss = self.model.forward(x_batch, t_batch)\n",
    "                self.model.backward()\n",
    "                for param, grad in zip(self.model.params, self.model.grads):\n",
    "                    self.optimizer.update(param, grad)\n",
    "                loss_total += loss\n",
    "            eval_loss = loss_total/max_iter\n",
    "            print('LOSS: {}'.format(eval_loss))\n",
    "            self.loss_lst += [eval_loss]\n",
    "    def plot(self):\n",
    "        plt.plot(self.loss_lst)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6f70273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinjy\\anaconda3\\envs\\machine-learning\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 1.0641172706124489\n",
      "LOSS: 0.8372360491212565\n",
      "LOSS: 0.7794947490517309\n",
      "LOSS: 0.7978600233747667\n",
      "LOSS: 0.7806210859875915\n",
      "LOSS: 0.7818726515377838\n",
      "LOSS: 0.7806685135594191\n",
      "LOSS: 0.7591338918104458\n",
      "LOSS: 0.7733752668820753\n",
      "LOSS: 0.800526704556942\n",
      "LOSS: 0.7645329429659742\n",
      "LOSS: 0.7574696400511407\n",
      "LOSS: 0.7513228663069971\n",
      "LOSS: 0.8234371487124182\n",
      "LOSS: 0.7637452030823756\n",
      "LOSS: 0.7329505896385877\n",
      "LOSS: 0.7385681934007557\n",
      "LOSS: 0.7508365534644399\n",
      "LOSS: 0.7373401026758372\n",
      "LOSS: 0.7331747641575125\n",
      "LOSS: 0.7338476249970937\n",
      "LOSS: 0.7671566571470934\n",
      "LOSS: 0.728014320544483\n",
      "LOSS: 0.7234408773029753\n",
      "LOSS: 0.7391866496500696\n",
      "LOSS: 0.7216764898879716\n",
      "LOSS: 0.7137016707625599\n",
      "LOSS: 0.7321803362047781\n",
      "LOSS: 0.7168739271831351\n",
      "LOSS: 0.6992351009718065\n",
      "LOSS: 0.7186205365498985\n",
      "LOSS: 0.7182552696810329\n",
      "LOSS: 0.7054867163925354\n",
      "LOSS: 0.7048377124918783\n",
      "LOSS: 0.6817046019828992\n",
      "LOSS: 0.6949608470676905\n",
      "LOSS: 0.6926500406659526\n",
      "LOSS: 0.6714769111975282\n",
      "LOSS: 0.6793756301271008\n",
      "LOSS: 0.6955563062058965\n",
      "LOSS: 0.6681674835013757\n",
      "LOSS: 0.637637927111945\n",
      "LOSS: 0.6453454831258003\n",
      "LOSS: 0.6564860500144266\n",
      "LOSS: 0.6395317179576108\n",
      "LOSS: 0.6200073831872934\n",
      "LOSS: 0.6246834784607201\n",
      "LOSS: 0.6157092140534604\n",
      "LOSS: 0.6050023953073047\n",
      "LOSS: 0.6089794640025246\n",
      "LOSS: 0.5818969272433548\n",
      "LOSS: 0.5794908029549156\n",
      "LOSS: 0.5823357660036859\n",
      "LOSS: 0.591568179045415\n",
      "LOSS: 0.5655200562579286\n",
      "LOSS: 0.5617802854132583\n",
      "LOSS: 0.5607701932349382\n",
      "LOSS: 0.5373606686062692\n",
      "LOSS: 0.5375912068384118\n",
      "LOSS: 0.5385953592969656\n",
      "LOSS: 0.5143672628179117\n",
      "LOSS: 0.5079006613605996\n",
      "LOSS: 0.5067183175942622\n",
      "LOSS: 0.4939982544013435\n",
      "LOSS: 0.48713450329408137\n",
      "LOSS: 0.4739119501831769\n",
      "LOSS: 0.4831521514638215\n",
      "LOSS: 0.4671268802335886\n",
      "LOSS: 0.4663655234009316\n",
      "LOSS: 0.44147332656006427\n",
      "LOSS: 0.4400285776723291\n",
      "LOSS: 0.4299057692076699\n",
      "LOSS: 0.418939986244251\n",
      "LOSS: 0.42027097146316883\n",
      "LOSS: 0.41316291250204573\n",
      "LOSS: 0.39874558908177415\n",
      "LOSS: 0.3931365250434823\n",
      "LOSS: 0.3869010981003683\n",
      "LOSS: 0.3950698309962696\n",
      "LOSS: 0.3821779378538333\n",
      "LOSS: 0.3728744984274798\n",
      "LOSS: 0.3631747270067184\n",
      "LOSS: 0.3570636307407852\n",
      "LOSS: 0.34660415472178013\n",
      "LOSS: 0.3461902032807191\n",
      "LOSS: 0.3337519357407979\n",
      "LOSS: 0.3253541252107923\n",
      "LOSS: 0.3170234860586228\n",
      "LOSS: 0.3115832183008494\n",
      "LOSS: 0.3213606233871065\n",
      "LOSS: 0.31126571675745834\n",
      "LOSS: 0.3023914767627248\n",
      "LOSS: 0.2982092116819789\n",
      "LOSS: 0.30005776370539927\n",
      "LOSS: 0.2928027245579317\n",
      "LOSS: 0.28478084274264803\n",
      "LOSS: 0.2827238633439291\n",
      "LOSS: 0.2807093233959794\n",
      "LOSS: 0.2752617898102288\n",
      "LOSS: 0.27431683543979135\n",
      "LOSS: 0.2670410652337874\n",
      "LOSS: 0.26105206934186587\n",
      "LOSS: 0.2659712851145154\n",
      "LOSS: 0.26068963449171606\n",
      "LOSS: 0.2591309486124288\n",
      "LOSS: 0.2571018312755825\n",
      "LOSS: 0.24299741747068335\n",
      "LOSS: 0.2475487081644713\n",
      "LOSS: 0.24501742051674252\n",
      "LOSS: 0.23931116425764545\n",
      "LOSS: 0.2439896260314872\n",
      "LOSS: 0.23568890441371812\n",
      "LOSS: 0.23604091123963186\n",
      "LOSS: 0.23636047028615992\n",
      "LOSS: 0.23030290144330223\n",
      "LOSS: 0.23204319751032734\n",
      "LOSS: 0.22870758468299854\n",
      "LOSS: 0.22788465718127432\n",
      "LOSS: 0.22929312290209677\n",
      "LOSS: 0.21617616264581865\n",
      "LOSS: 0.2221607603237068\n",
      "LOSS: 0.21172354199034044\n",
      "LOSS: 0.21120681330709096\n",
      "LOSS: 0.20875888192758651\n",
      "LOSS: 0.2088671544775591\n",
      "LOSS: 0.20408925397422872\n",
      "LOSS: 0.21087471509406158\n",
      "LOSS: 0.2058410041812951\n",
      "LOSS: 0.20740703312598932\n",
      "LOSS: 0.19987340791621871\n",
      "LOSS: 0.21232014411173777\n",
      "LOSS: 0.1995272689774475\n",
      "LOSS: 0.20170870105383285\n",
      "LOSS: 0.19722595784917535\n",
      "LOSS: 0.1991583178156648\n",
      "LOSS: 0.1947314901947337\n",
      "LOSS: 0.19510755656739504\n",
      "LOSS: 0.1947240372465741\n",
      "LOSS: 0.19952436553854658\n",
      "LOSS: 0.19418036148202336\n",
      "LOSS: 0.19393345708800205\n",
      "LOSS: 0.19017819338047054\n",
      "LOSS: 0.18856987578030815\n",
      "LOSS: 0.19608015487039215\n",
      "LOSS: 0.1908654041821474\n",
      "LOSS: 0.1803628403834851\n",
      "LOSS: 0.1856879400060769\n",
      "LOSS: 0.1900436081888569\n",
      "LOSS: 0.18172616632161326\n",
      "LOSS: 0.1773987580858843\n",
      "LOSS: 0.19245398025204422\n",
      "LOSS: 0.18194069548128605\n",
      "LOSS: 0.1838844991285744\n",
      "LOSS: 0.18709003609453922\n",
      "LOSS: 0.17325583168717398\n",
      "LOSS: 0.17575541578963597\n",
      "LOSS: 0.17700181759013583\n",
      "LOSS: 0.1689077329911362\n",
      "LOSS: 0.17863835060494487\n",
      "LOSS: 0.1697830273043965\n",
      "LOSS: 0.1660685679881292\n",
      "LOSS: 0.17035955988997248\n",
      "LOSS: 0.16597290601196085\n",
      "LOSS: 0.17172324702199596\n",
      "LOSS: 0.16990607506666416\n",
      "LOSS: 0.17024498299768487\n",
      "LOSS: 0.16776796006208833\n",
      "LOSS: 0.1608651729015121\n",
      "LOSS: 0.17041066434669533\n",
      "LOSS: 0.16423467260155442\n",
      "LOSS: 0.16255660509373826\n",
      "LOSS: 0.15881365472244224\n",
      "LOSS: 0.15554298119903862\n",
      "LOSS: 0.15756166395952526\n",
      "LOSS: 0.15752074203452013\n",
      "LOSS: 0.15867598009188613\n",
      "LOSS: 0.16310863617037316\n",
      "LOSS: 0.15609841073239764\n",
      "LOSS: 0.1526808929125935\n",
      "LOSS: 0.15743165840188983\n",
      "LOSS: 0.15522227808521197\n",
      "LOSS: 0.1627880931046515\n",
      "LOSS: 0.14744806606375865\n",
      "LOSS: 0.15265907362856904\n",
      "LOSS: 0.14848258455891364\n",
      "LOSS: 0.15045709533750837\n",
      "LOSS: 0.15114087205783244\n",
      "LOSS: 0.15014919890607917\n",
      "LOSS: 0.14126630955119196\n",
      "LOSS: 0.14934983196169813\n",
      "LOSS: 0.14703106430513257\n",
      "LOSS: 0.14617073828307964\n",
      "LOSS: 0.1415600048877922\n",
      "LOSS: 0.1431180294843673\n",
      "LOSS: 0.14815180176075435\n",
      "LOSS: 0.1371967181079858\n",
      "LOSS: 0.14096896846445497\n",
      "LOSS: 0.13833534388357566\n",
      "LOSS: 0.1397916329558788\n",
      "LOSS: 0.13583763186793088\n",
      "LOSS: 0.14035335535748583\n",
      "LOSS: 0.14311804418051358\n",
      "LOSS: 0.13668773534101822\n",
      "LOSS: 0.1351291480357587\n",
      "LOSS: 0.13503481325093042\n",
      "LOSS: 0.1408289972411432\n",
      "LOSS: 0.1338505138224751\n",
      "LOSS: 0.13536954140365512\n",
      "LOSS: 0.1399111383351049\n",
      "LOSS: 0.1326458859361929\n",
      "LOSS: 0.13268574457383264\n",
      "LOSS: 0.12528247535539055\n",
      "LOSS: 0.1346938915637957\n",
      "LOSS: 0.13265906798923102\n",
      "LOSS: 0.13385623404240726\n",
      "LOSS: 0.12700510709743826\n",
      "LOSS: 0.12660793293826939\n",
      "LOSS: 0.1257093773890753\n",
      "LOSS: 0.12687446804447877\n",
      "LOSS: 0.12517959505952342\n",
      "LOSS: 0.12458486042001563\n",
      "LOSS: 0.12043085780121435\n",
      "LOSS: 0.12852772443326563\n",
      "LOSS: 0.1275126144570286\n",
      "LOSS: 0.12829418091001443\n",
      "LOSS: 0.120950528642332\n",
      "LOSS: 0.12413822248292514\n",
      "LOSS: 0.11959985416272507\n",
      "LOSS: 0.12385456407988335\n",
      "LOSS: 0.1214971640218188\n",
      "LOSS: 0.11789718974966482\n",
      "LOSS: 0.1188870655037794\n",
      "LOSS: 0.1258528926901633\n",
      "LOSS: 0.12271030324609056\n",
      "LOSS: 0.11941401564902805\n",
      "LOSS: 0.11821116840576366\n",
      "LOSS: 0.11730414529929427\n",
      "LOSS: 0.11805925819383598\n",
      "LOSS: 0.11363723748114082\n",
      "LOSS: 0.11304774698116016\n",
      "LOSS: 0.11502142890322058\n",
      "LOSS: 0.12569238416881276\n",
      "LOSS: 0.11374811869893194\n",
      "LOSS: 0.11520333067870185\n",
      "LOSS: 0.11593184649529517\n",
      "LOSS: 0.11654043398293754\n",
      "LOSS: 0.1208606394516329\n",
      "LOSS: 0.11554404155963945\n",
      "LOSS: 0.11275219362556119\n",
      "LOSS: 0.11680618914330773\n",
      "LOSS: 0.11079349708671413\n",
      "LOSS: 0.11559048668322743\n",
      "LOSS: 0.11507638436533471\n",
      "LOSS: 0.11023057788495025\n",
      "LOSS: 0.11165022434731715\n",
      "LOSS: 0.11142284309000375\n",
      "LOSS: 0.11356622802056318\n",
      "LOSS: 0.10770881605739775\n",
      "LOSS: 0.10715752213595811\n",
      "LOSS: 0.1116177623808011\n",
      "LOSS: 0.1118951801389263\n",
      "LOSS: 0.10647838469370023\n",
      "LOSS: 0.11064554734128398\n",
      "LOSS: 0.11233772465662473\n",
      "LOSS: 0.10950451321013807\n",
      "LOSS: 0.10816427345056023\n",
      "LOSS: 0.10305884934166669\n",
      "LOSS: 0.10272757390536562\n",
      "LOSS: 0.1061466919718926\n",
      "LOSS: 0.10560953924110092\n",
      "LOSS: 0.1085970069458432\n",
      "LOSS: 0.1066452302395203\n",
      "LOSS: 0.10472511237072454\n",
      "LOSS: 0.10509190718787056\n",
      "LOSS: 0.10262653977657057\n",
      "LOSS: 0.10051919796922397\n",
      "LOSS: 0.10499684395298692\n",
      "LOSS: 0.09860599346814616\n",
      "LOSS: 0.10660309244968998\n",
      "LOSS: 0.10550813514085026\n",
      "LOSS: 0.10303746029265784\n",
      "LOSS: 0.1013282764983805\n",
      "LOSS: 0.10145030045083059\n",
      "LOSS: 0.1013266111955217\n",
      "LOSS: 0.09913531177752279\n",
      "LOSS: 0.10120764762642276\n",
      "LOSS: 0.10346207095766968\n",
      "LOSS: 0.10118284860738028\n",
      "LOSS: 0.09560259857967379\n",
      "LOSS: 0.09718805833102616\n",
      "LOSS: 0.10137902348845389\n",
      "LOSS: 0.10186827905767029\n",
      "LOSS: 0.09959433530790705\n",
      "LOSS: 0.1003421900722353\n",
      "LOSS: 0.0959622006884506\n",
      "LOSS: 0.09708855134099023\n",
      "LOSS: 0.09782651912056477\n",
      "LOSS: 0.09949829595668465\n",
      "LOSS: 0.09386243603683639\n",
      "LOSS: 0.09597384446779608\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "trainer = Trainer(TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3), SGD(lr=learning_rate))\n",
    "trainer.fit(*load_data(), 300, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d341a5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
